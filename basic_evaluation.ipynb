{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this assignment is to create a basic program that provides an overview of basic evaluation metrics (in particular, precision, recall, f-score and a confusion matrix) from documents provided in the conll format. \n",
    "You will need to implement the calculations for precision, recall and f-score yourself (i.e. do not use an existing module that spits them out). Make sure that your code can handle the situation where there are no true positives for a specific class.\n",
    "\n",
    "This notebook provides functions for reading in conll structures with pandas and proposes a structure for calculating your evaluation metrics and producing the confusion matrix. Feel free to adjust the proposed structure if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "# see tips & tricks on using defaultdict (remove when you do not use it)\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note Pandas\n",
    "\n",
    "Pandas is a module that provides data structures and is widely used for dealing with data representations in machine learning. It is a bit more advanced than the csv module we saw in the preprocessing notebook.\n",
    "Working with pandas data structures can be tricky, but it will generally work well if you follow online tutorials and examples closely. If your code is slow before you even started training your models, it is likely to be a problem with the way you are using Pandas (it will still work in most cases, you will just have to wait a bit longer). Once you are more used to working with modules and complex objects, it will also become easier to work with Pandas.\n",
    "\n",
    "In the examples below, we assume that the data representations that are used have headers (i.e. specific titles that indicate what information can be found in each column of the conll file). You can look at the mini- sample files in data to get an idea of how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to exclude the O class from the tables (for the report)\n",
    "exclude_O_class = True\n",
    "\n",
    "# Option to print latex tables\n",
    "print_latex = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annotations(inputfile, annotationcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    This function extracts annotations represented in the conll format from a file\n",
    "    \n",
    "    :param inputfile: the path to the conll file\n",
    "    :param annotationcolumn: the name of the column in which the target annotation is provided\n",
    "    :param delimiter: optional parameter to overwrite the default delimiter (tab)\n",
    "    :type inputfile: string\n",
    "    :type annotationcolumn: string\n",
    "    :type delimiter: string\n",
    "    :returns: the annotations as a list\n",
    "    '''\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    conll_input = pd.read_csv(inputfile, sep=delimiter, quotechar=delimiter, header=None, on_bad_lines='skip')\n",
    "    annotations = conll_input[int(annotationcolumn)].tolist()\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_counts(goldannotations, machineannotations):\n",
    "    '''\n",
    "    This function compares the gold annotations to machine output\n",
    "    \n",
    "    :param goldannotations: the gold annotations\n",
    "    :param machineannotations: the output annotations of the system in question\n",
    "    :type goldannotations: the type of the object created in extract_annotations\n",
    "    :type machineannotations: the type of the object created in extract_annotations\n",
    "    \n",
    "    :returns: a countainer providing the counts for each predicted and gold class pair\n",
    "    '''\n",
    "    \n",
    "    # TIP on how to get the counts for each class\n",
    "    # https://stackoverflow.com/questions/49393683/how-to-count-items-in-a-nested-dictionary, last accessed 22.10.2020\n",
    "    evaluation_counts = defaultdict(Counter)\n",
    "    for i in range(len(goldannotations)):\n",
    "        evaluation_counts[goldannotations[i]][machineannotations[i]] += 1\n",
    "    \n",
    "    return evaluation_counts            \n",
    "\n",
    "def provide_confusion_matrix(evaluation_counts):\n",
    "    '''\n",
    "    Read in the evaluation counts and provide a confusion matrix for each class\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns a confusion matrix\n",
    "    '''\n",
    "    confusion_dict = dict()\n",
    "    for gold_key in evaluation_counts.keys():\n",
    "        confusion_dict[gold_key] = evaluation_counts[gold_key]\n",
    "        \n",
    "    confusion_matrix = pd.DataFrame.from_dict(confusion_dict, orient='index').fillna(0)\n",
    "\n",
    "    # Drop the O class if required\n",
    "    if exclude_O_class:\n",
    "        confusion_matrix = confusion_matrix.drop(columns=['O'])\n",
    "        confusion_matrix = confusion_matrix.drop('O')\n",
    "        \n",
    "    return confusion_matrix\n",
    "\n",
    "    \n",
    "def tp_tn_fp_fn(confusion_matrix, class_name):\n",
    "    '''\n",
    "    Calculate the number of true positives (tp), true negatives (tn), false positives (fp) and false negatives (fn) from a confusion matrix and return them in a dictionary\n",
    "    \n",
    "    :param confusion_matrix: the confusion matrix\n",
    "    :param class_name: the name of the class for which the tp, tn, fp and fn are requested\n",
    "    :type confusion_matrix: a pandas DataFrame (as returned by provide_confusion_matrix)\n",
    "    :type class_name: string\n",
    "    \n",
    "    :returns a dictionary of tp, tn, fp and fn\n",
    "    '''\n",
    "    tp = confusion_matrix.loc[class_name, class_name]\n",
    "    fp = sum(confusion_matrix.loc[:, class_name].to_list()) - tp\n",
    "    fn = sum(confusion_matrix.loc[class_name, :].to_list()) - tp\n",
    "    tn = confusion_matrix.values.sum() - tp - fp - fn\n",
    "    \n",
    "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
    "    \n",
    "def calculate_precision_recall_fscore(evaluation_counts):\n",
    "    '''\n",
    "    Calculate precision recall and fscore for each class and return them in a dictionary\n",
    "    \n",
    "    :param evaluation_counts: a container from which you can obtain the true positives, false positives and false negatives for each class\n",
    "    :type evaluation_counts: type of object returned by obtain_counts\n",
    "    \n",
    "    :returns the precision, recall and f-score of each class in a container\n",
    "    '''\n",
    "    \n",
    "    precision_recall_fscore_dict = dict()\n",
    "    confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "    \n",
    "    for class_label in confusion_matrix.columns:\n",
    "        precision_recall_fscore_dict[class_label] = {}\n",
    "        \n",
    "        tp_tn_fp_fn_dict = tp_tn_fp_fn(confusion_matrix, class_label)\n",
    "        tp = tp_tn_fp_fn_dict['TP']\n",
    "        tn = tp_tn_fp_fn_dict['TN']\n",
    "        fp = tp_tn_fp_fn_dict['FP']\n",
    "        fn = tp_tn_fp_fn_dict['FN']\n",
    "        \n",
    "        precision = tp/(fp+tp)\n",
    "        recall = tp/(fn+tp)\n",
    "        \n",
    "        #Check if there are any True Postives\n",
    "        if tp == 0:\n",
    "            print(f\"Warning: Number of True Positives is 0 for class {class_label}\")\n",
    "            fscore = 0\n",
    "        else:\n",
    "            fscore = (2*precision*recall)/(precision+recall)\n",
    "        \n",
    "        precision_recall_fscore_dict[class_label]['precision'] = precision\n",
    "        precision_recall_fscore_dict[class_label]['recall'] = recall\n",
    "        precision_recall_fscore_dict[class_label]['f-score'] = fscore\n",
    "\n",
    "    return precision_recall_fscore_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carry_out_evaluation(gold_annotations, systemfile, systemcolumn, delimiter='\\t'):\n",
    "    '''\n",
    "    Carries out the evaluation process (from input file to calculating relevant scores)\n",
    "    \n",
    "    :param gold_annotations: list of gold annotations\n",
    "    :param systemfile: path to file with system output\n",
    "    :param systemcolumn: indication of column with relevant information\n",
    "    :param delimiter: specification of formatting of file (default delimiter set to '\\t')\n",
    "    \n",
    "    returns evaluation information for this specific system\n",
    "    '''\n",
    "    system_annotations = extract_annotations(systemfile, systemcolumn, delimiter)\n",
    "    evaluation_counts = obtain_counts(gold_annotations, system_annotations)\n",
    "    confusion_matrix = provide_confusion_matrix(evaluation_counts)\n",
    "\n",
    "    # Check if the table is to be printed in latex format\n",
    "    if print_latex:\n",
    "        print(\"gold rows, machine columns\\n\", confusion_matrix.to_latex())\n",
    "    else:\n",
    "        print(\"gold rows, machine columns\\n\", confusion_matrix)\n",
    "\n",
    "    evaluation_outcome = calculate_precision_recall_fscore(evaluation_counts)\n",
    "    \n",
    "    return evaluation_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macro_averages(evaluation_outcome, system_and_model_name):\n",
    "    \"\"\"\n",
    "    Calculate the macro averages (mean) of the precision, recall and f-scores over the classes.\n",
    "\n",
    "    :param evaluations_outcome: the outcome of evaluating a system\n",
    "    :param system_and_model_name: string, combination of the name of the current system and the model\n",
    "\n",
    "    :returns: a dictionary with the macro averages of precision, recall and f-score\n",
    "    \"\"\"\n",
    "    # initialise variables\n",
    "    sum_precision = 0\n",
    "    sum_recall = 0\n",
    "    sum_fscore = 0\n",
    "    count_classes = 0\n",
    "    macro_metrics_dict = dict()\n",
    "\n",
    "    # Sum metrics\n",
    "    for class_name in evaluation_outcome:\n",
    "        count_classes += 1\n",
    "        sum_precision += evaluation_outcome[class_name]['precision']\n",
    "        sum_recall += evaluation_outcome[class_name]['recall']\n",
    "        sum_fscore += evaluation_outcome[class_name]['f-score']\n",
    "\n",
    "    # Calculate macro average of metrics (divide sum by count)\n",
    "    macro_metrics_dict['precision'] = sum_precision / count_classes\n",
    "    macro_metrics_dict['recall'] = sum_recall / count_classes\n",
    "    macro_metrics_dict['f-score'] = sum_fscore / count_classes\n",
    "\n",
    "    # Create macro average dataframe\n",
    "    macro_avg_df = pd.DataFrame.from_dict({system_and_model_name: macro_metrics_dict}, orient='index')\n",
    "\n",
    "    # Check if the the table is to be printed in latex format\n",
    "    if print_latex:\n",
    "        print(\"\\nMacro averages\\n\", macro_avg_df.to_latex(float_format=\"%.3f\"))\n",
    "    else:\n",
    "        print(\"\\nMacro averages\\n\", macro_avg_df)\n",
    "\n",
    "    return macro_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_output_tables(evaluations):\n",
    "    '''\n",
    "    Create tables based on the evaluation of various systems\n",
    "    \n",
    "    :param evaluations: the outcome of evaluating one or more systems\n",
    "    '''\n",
    "    #https:stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "    evaluations_pddf = pd.DataFrame.from_dict({(i,j): evaluations[i][j]\n",
    "                                              for i in evaluations.keys()\n",
    "                                              for j in evaluations[i].keys()},\n",
    "                                             orient='index')\n",
    "    # Check if the table is to be printed in latex format\n",
    "    if print_latex:\n",
    "        print(\"\\nevaluations_pddf latex\\n\", evaluations_pddf.to_latex(float_format=\"%.3f\"))\n",
    "    else:\n",
    "        print(\"\\nevaluations_pddf\\n\", evaluations_pddf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations(goldfile, goldcolumn, systems):\n",
    "    '''\n",
    "    Carry out standard evaluation for one or more system outputs\n",
    "    \n",
    "    :param goldfile: path to file with goldstandard\n",
    "    :param goldcolumn: indicator of column in gold file where gold labels can be found\n",
    "    :param systems: required information to find and process system output\n",
    "    :type goldfile: string\n",
    "    :type goldcolumn: integer\n",
    "    :type systems: list (providing file name, information on tab with system output and system name for each element)\n",
    "    \n",
    "    :returns the evaluations for all systems\n",
    "    '''\n",
    "    evaluations = {}\n",
    "    #not specifying delimiters here, since it corresponds to the default ('\\t')\n",
    "    gold_annotations = extract_annotations(goldfile, goldcolumn)\n",
    "    for system in systems:\n",
    "        sys_evaluation = carry_out_evaluation(gold_annotations, system[0], system[1])\n",
    "        evaluations[system[2]] = sys_evaluation\n",
    "        calculate_macro_averages(sys_evaluation, system[2])\n",
    "        \n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the overall set-up\n",
    "\n",
    "The functions below illustrate how to run the setup as outlined above using a main function and, later, commandline arguments. This setup will facilitate the transformation to an experimental setup that no longer makes use of notebooks, that you will submit later on. There are also some functions that can be used to test your implementation You can carry out a few small tests yourself with the data provided in the data/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_evaluation_value(system, class_label, value_name, evaluations):\n",
    "    '''\n",
    "    Return the outcome of a specific value of the evaluation\n",
    "    \n",
    "    :param system: the name of the system\n",
    "    :param class_label: the name of the class for which the value should be returned\n",
    "    :param value_name: the name of the score that is returned\n",
    "    :param evaluations: the overview of evaluations\n",
    "    \n",
    "    :returns the requested value\n",
    "    '''\n",
    "    return evaluations[system][class_label][value_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_information(system_information):\n",
    "    '''\n",
    "    Takes system information in the form that it is passed on through sys.argv or via a settingsfile\n",
    "    and returns a list of elements specifying all the needed information on each system output file to carry out the evaluation.\n",
    "    \n",
    "    :param system_information is the input as from a commandline or an input file\n",
    "    '''\n",
    "    # https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "    systems_list = [system_information[i:i + 3] for i in range(0, len(system_information), 3)]\n",
    "    return systems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(my_args=None):\n",
    "    '''\n",
    "    A main function. This does not make sense for a notebook, but it is here as an example.\n",
    "    sys.argv is a very lightweight way of passing arguments from the commandline to a script.\n",
    "    '''\n",
    "    if my_args is None:\n",
    "        my_args = sys.argv\n",
    "    system_info = create_system_information(my_args[2:])\n",
    "    evaluations = run_evaluations(my_args[0], my_args[1], system_info)\n",
    "    provide_output_tables(evaluations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change paths\n",
    "The cell below can be used to evaluate the output of a classifier. If the file paths are not the same on your device, please replace the path of the files and their respective NE label column identifiers in the cell below to be able to run the code. The name of the system that was used to classify the NE labels should also be changed in the system_name variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold rows, machine columns\n",
      "           PERSON   ORG  MISC  LOCATION\n",
      "ORG          196  1098    72       247\n",
      "LOCATION      80   134    37      1647\n",
      "MISC          53   154   767        71\n",
      "PERSON      2336   321    51       107\n",
      "\n",
      "Macro averages\n",
      "        precision    recall   f-score\n",
      "SpaCy   0.785516  0.778072  0.780403\n",
      "\n",
      "evaluations_pddf\n",
      "                 precision    recall   f-score\n",
      "SpaCy PERSON     0.876548  0.829840  0.852555\n",
      "      ORG        0.643234  0.680719  0.661446\n",
      "      MISC       0.827400  0.733971  0.777890\n",
      "      LOCATION   0.794884  0.867756  0.829723\n"
     ]
    }
   ],
   "source": [
    "# Replace the input paths and their respective column identifiers for the NE labels here.\n",
    "path_goldfile = '..\\data\\conll2003.dev-preprocessed.conll'\n",
    "gold_column_identifier = 3 #3 for proprocessed conll files (with added features)\n",
    "path_systemfile = '..\\data\\spacy_out.dev-preprocessed.conll'\n",
    "system_column_identifier = 2 #8 for preprocessed conll files with added features\n",
    "system_name = 'SpaCy'\n",
    "\n",
    "my_args = [path_goldfile, str(gold_column_identifier), path_systemfile, str(system_column_identifier), system_name]\n",
    "\n",
    "main(my_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
